{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "\n",
    "### As a reminder:\n",
    "When our activation function is a Sigmoid function, the Perceptron takes the inputs and multiplies them by the weights in the edges and adds the results, then applies the sigmoid function. So instead of returning 1 and 0 like before, it returns values between zero and 1, such as .99 or .67 etc.  Now, it says the probabilty of a point is - 45 percent for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function is defined as sigmoid(x) = 1/(1+e-x). If the score is defined by 4x1 + 5x2 - 9 = score, then which of the following points has exactly a 50% probability of being blue or red?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999979388463\n",
      "2.0611536181902037e-09\n",
      "0.8807970779778823\n",
      "0.11920292202211755\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import logistic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid_array(x):\n",
    "    \"\"\"The Sigmoid function used for binary classification in logistic regression model.\n",
    "    While creating artificial neurons sigmoid function used as the activation function.\n",
    "    param inputs: array\n",
    "    return: sigmoid array\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Testing:\n",
    "# print(sigmoid_array(0)) # (1,1)\n",
    "# print(sigmoid_array(19)) # (2,4)\n",
    "# print(sigmoid_array(-14)) # (5,-5)\n",
    "# print(sigmoid_array(0)) # (-4,5)\n",
    "#10x1 + 10x2 + b\n",
    "# x1 + x2 + b\n",
    "# 1,1\n",
    "# -1,-1\n",
    "\n",
    "print(sigmoid_array(20))\n",
    "print(sigmoid_array(-20))\n",
    "print(sigmoid_array(2))\n",
    "print(sigmoid_array(-2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftMax Function\n",
    "\n",
    "### The SoftMax Function is equivalent to the Sigmoid Activation function, except it works with problems with 3 or more classes.\n",
    "\n",
    "    # Softmax function calculates the probabilities distribution of the event over ‘n’ different events. In general way of saying, this function will calculate the probabilities of each target class over all possible target classes. Later the calculated probabilities will be helpful for determining the target class for the given inputs.\n",
    "\n",
    "    # The main advantage of using Softmax is the output probabilities range. The range will 0 to 1, and the sum of all the probabilities will be equal to one. If the softmax function used for multi-classification model it returns the probabilities of each class and the target class will have the high probability.\n",
    "\n",
    "    # The formula computes the exponential (e-power) of the given input value and the sum of exponential values of all the values in the inputs. Then the ratio of the exponential of the input value and the sum of exponential values is the output of the softmax function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057317038046, 0.24472847105479764, 0.6652409557748219]\n"
     ]
    }
   ],
   "source": [
    "# Softmax Activation Function\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    each = [np.exp(num) / sum(np.exp(L)) for num in L]\n",
    "    L = each\n",
    "    \n",
    "    return L\n",
    "print(softmax([5,6,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.828313737302301\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy with 2 classes 1 or 0\n",
    "# cross entropy is the sums of the negatives of the logarithms of the probabilities of the points being their colors\n",
    "# the smaller the cross entropy the better\n",
    "\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))\n",
    "Y = [1, 0, 1, 1]\n",
    "P = [0.4, 0.6, 0.1, 0.5]\n",
    "# print(cross_entropy(Y,P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions for plotting and drawing lines\n",
    "\n",
    "def plot_points(X, y):\n",
    "    admitted = X[np.argwhere(y==1)]\n",
    "    rejected = X[np.argwhere(y==0)]\n",
    "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')\n",
    "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')\n",
    "\n",
    "def display(m, b, color='g--'):\n",
    "    plt.xlim(-0.05,1.05)\n",
    "    plt.ylim(-0.05,1.05)\n",
    "    x = np.arange(-10, 10, 0.1)\n",
    "    plt.plot(x, m*x+b, color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', header=None)\n",
    "X = np.array(data[[0,1]])\n",
    "y = np.array(data[2])\n",
    "plot_points(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basic functions\n",
    "\n",
    "\n",
    "    Sigmoid activation function\n",
    "\n",
    "𝜎(𝑥)=11+𝑒−𝑥\n",
    "\n",
    "    Output (prediction) formula\n",
    "\n",
    "𝑦̂ =𝜎(𝑤1𝑥1+𝑤2𝑥2+𝑏)\n",
    "\n",
    "    Error function\n",
    "\n",
    "𝐸𝑟𝑟𝑜𝑟(𝑦,𝑦̂ )=−𝑦log(𝑦̂ )−(1−𝑦)log(1−𝑦̂ )\n",
    "\n",
    "    The function that updates the weights\n",
    "\n",
    "𝑤𝑖⟶𝑤𝑖+𝛼(𝑦−𝑦̂ )𝑥𝑖\n",
    "\n",
    "𝑏⟶𝑏+𝛼(𝑦−𝑦̂ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the following functions\n",
    "\n",
    "# Activation (sigmoid) function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Output (prediction) formula\n",
    "def output_formula(features, weights, bias):\n",
    "    return sigmoid(np.dot(features, weights) + bias)\n",
    "\n",
    "# Error (log-loss) formula\n",
    "def error_formula(y, output):\n",
    "    return -y * np.log(output) - (1-y) * np.log(1-output)\n",
    "\n",
    "# Gradient descent step\n",
    "def update_weights(x, y, weights, bias, learnrate):\n",
    "    output = output_formula(x, weights, bias)\n",
    "    d_error = y - output\n",
    "    weights += learnrate * d_error * x\n",
    "    bias += learnrate * d_error\n",
    "    return weights, bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training function\n",
    "\n",
    "This function will help us iterate the gradient descent algorithm through all the data, for a number of epochs. It will also plot the data, and some of the boundary lines obtained as we run the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(44)\n",
    "\n",
    "epochs = 100\n",
    "learnrate = 0.01\n",
    "\n",
    "def train(features, targets, epochs, learnrate, graph_lines=False):\n",
    "    \n",
    "    errors = []\n",
    "    n_records, n_features = features.shape\n",
    "    last_loss = None\n",
    "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "    bias = 0\n",
    "    for e in range(epochs):\n",
    "        del_w = np.zeros(weights.shape)\n",
    "        for x, y in zip(features, targets):\n",
    "            output = output_formula(x, weights, bias)\n",
    "            error = error_formula(y, output)\n",
    "            weights, bias = update_weights(x, y, weights, bias, learnrate)\n",
    "        \n",
    "        # Printing out the log-loss error on the training set\n",
    "        out = output_formula(features, weights, bias)\n",
    "        loss = np.mean(error_formula(targets, out))\n",
    "        errors.append(loss)\n",
    "        if e % (epochs / 10) == 0:\n",
    "            print(\"\\n========== Epoch\", e,\"==========\")\n",
    "            if last_loss and last_loss < loss:\n",
    "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "            else:\n",
    "                print(\"Train loss: \", loss)\n",
    "            last_loss = loss\n",
    "            predictions = out > 0.5\n",
    "            accuracy = np.mean(predictions == targets)\n",
    "            print(\"Accuracy: \", accuracy)\n",
    "        if graph_lines and e % (epochs / 100) == 0:\n",
    "            display(-weights[0]/weights[1], -bias/weights[1])\n",
    "            \n",
    "\n",
    "    # Plotting the solution boundary\n",
    "    plt.title(\"Solution boundary\")\n",
    "    display(-weights[0]/weights[1], -bias/weights[1], 'black')\n",
    "\n",
    "    # Plotting the data\n",
    "    plot_points(features, targets)\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the error\n",
    "    plt.title(\"Error Plot\")\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.ylabel('Error')\n",
    "    plt.plot(errors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the algorithm!\n",
    "\n",
    "When we run the function, we'll obtain the following:\n",
    "\n",
    "    10 updates with the current training loss and accuracy\n",
    "    A plot of the data and some of the boundary lines obtained. The final one is in black. Notice how the lines get closer and closer to the best fit, as we go through more epochs.\n",
    "    A plot of the error function. Notice how it decreases as we go through more epochs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(X, y, epochs, learnrate, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
